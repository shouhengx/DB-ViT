{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import scipy.io as sio\n",
    "from scipy.io import savemat\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from vit_pytorch import ViT\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchsummary import summary\n",
    "import geniter\n",
    "import record\n",
    "import Utils\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_DATASET = 'KSC'  # UP,IN,SV, KSC\n",
    "PARAM_EPOCH = 100\n",
    "PARAM_ITER = 3\n",
    "PATCH_SIZE = 6\n",
    "PARAM_VAL = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341]\n",
    "#global Dataset  # UP,IN,SV, KSC\n",
    "dataset = PARAM_DATASET  # input('Please input the name of Dataset(IN, UP, SV, KSC):')\n",
    "Dataset = dataset.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(Dataset, split=0.9):\n",
    "    data_path = '../../data/'\n",
    "    if Dataset == 'IN':\n",
    "        mat_data = sio.loadmat(data_path + 'Indian_pines_corrected.mat')\n",
    "        mat_gt = sio.loadmat(data_path + 'Indian_pines_gt.mat')\n",
    "        data_hsi = mat_data['indian_pines_corrected']\n",
    "        gt_hsi = mat_gt['indian_pines_gt']\n",
    "        K = 200\n",
    "        TOTAL_SIZE = 10249\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "    if Dataset == 'UP':\n",
    "        uPavia = sio.loadmat(data_path + 'PaviaU.mat')\n",
    "        gt_uPavia = sio.loadmat(data_path + 'PaviaU_gt.mat')\n",
    "        data_hsi = uPavia['paviaU']\n",
    "        gt_hsi = gt_uPavia['paviaU_gt']\n",
    "        K = 103\n",
    "        TOTAL_SIZE = 42776\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "    if Dataset == 'SV':\n",
    "        SV = sio.loadmat(data_path + 'Salinas_corrected.mat')\n",
    "        gt_SV = sio.loadmat(data_path + 'Salinas_gt.mat')\n",
    "        data_hsi = SV['salinas_corrected']\n",
    "        gt_hsi = gt_SV['salinas_gt']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 54129\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "    if Dataset == 'KSC':\n",
    "        KSV = sio.loadmat(data_path + 'KSC.mat')\n",
    "        gt_KSV = sio.loadmat(data_path + 'KSC_gt.mat')\n",
    "        data_hsi = KSV['KSC']\n",
    "        gt_hsi = gt_KSV['KSC_gt']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 5211\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "        \n",
    "    if Dataset == 'BO':\n",
    "        BO = sio.loadmat(data_path + 'Botswana.mat')\n",
    "        gt_BO = sio.loadmat(data_path + 'Botswana_gt.mat')\n",
    "        data_hsi = BO['Botswana']\n",
    "        gt_hsi = gt_BO['Botswana_gt']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 3248\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "    shapeor = data_hsi.shape\n",
    "    data_hsi = data_hsi.reshape(-1, data_hsi.shape[-1])\n",
    "    data_hsi = PCA(n_components=K).fit_transform(data_hsi)\n",
    "    shapeor = np.array(shapeor)\n",
    "    shapeor[-1] = K\n",
    "    data_hsi = data_hsi.reshape(shapeor)\n",
    "\n",
    "    return data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT = load_dataset(\n",
    "    Dataset, PARAM_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x, image_y, BAND = data_hsi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314368, 176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_hsi.reshape(\n",
    "    np.prod(data_hsi.shape[:2]), np.prod(data_hsi.shape[2:]))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 614)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_hsi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = gt_hsi.reshape(np.prod(gt_hsi.shape[:2]), )\n",
    "gt.shape\n",
    "CLASSES_NUM = max(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_LENGTH = PATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 2 * PATCH_LENGTH + 1\n",
    "img_cols = 2 * PATCH_LENGTH + 1\n",
    "img_channels = data_hsi.shape[2]\n",
    "INPUT_DIMENSION = data_hsi.shape[2]\n",
    "ALL_SIZE = data_hsi.shape[0] * data_hsi.shape[1]\n",
    "VAL_SIZE = int(TRAIN_SIZE)\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing.scale(data)\n",
    "data_ = data.reshape(data_hsi.shape[0], data_hsi.shape[1], data_hsi.shape[2])\n",
    "whole_data = data_\n",
    "padded_data = np.lib.pad(\n",
    "    whole_data, ((PATCH_LENGTH, PATCH_LENGTH), (PATCH_LENGTH, PATCH_LENGTH),\n",
    "                 (0, 0)),\n",
    "    'constant',\n",
    "    constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(proportion, ground_truth):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    labels_loc = {}\n",
    "    m = max(ground_truth)\n",
    "    for i in range(m):\n",
    "        indexes = [\n",
    "            j for j, x in enumerate(ground_truth.ravel().tolist())\n",
    "            if x == i + 1\n",
    "        ]\n",
    "        np.random.shuffle(indexes)\n",
    "        labels_loc[i] = indexes\n",
    "        if proportion != 1:\n",
    "            nb_val = max(int((1 - proportion) * len(indexes)), 3)\n",
    "        else:\n",
    "            nb_val = 0\n",
    "        train[i] = indexes[:nb_val]\n",
    "        test[i] = indexes[nb_val:]\n",
    "    train_indexes = []\n",
    "    test_indexes = []\n",
    "    for i in range(m):\n",
    "        train_indexes += train[i]\n",
    "        test_indexes += test[i]\n",
    "    np.random.shuffle(train_indexes)\n",
    "    np.random.shuffle(test_indexes)\n",
    "    return train_indexes, test_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Selecting Small Pieces from the Original Cube Data-----\n",
      "Train size:  (156, 13, 13, 145)\n",
      "Test size:  (2936, 13, 13, 145)\n",
      "Validation size:  (156, 13, 13, 145)\n"
     ]
    }
   ],
   "source": [
    "index_iter=0\n",
    "np.random.seed(seeds[index_iter])\n",
    "train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "_, total_indices = sampling(1, gt)\n",
    "\n",
    "TRAIN_SIZE = len(train_indices)\n",
    "#print('Train size: ', TRAIN_SIZE)\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "#print('Test size: ', TEST_SIZE)\n",
    "VAL_SIZE = int(TRAIN_SIZE)\n",
    "#print('Validation size: ', VAL_SIZE)\n",
    "\n",
    "print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
    "x_train,y_train, x_val,y_val, x_test,y_test, all_data, gt_all = geniter.generate_iter(\n",
    "        TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
    "        total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
    "        INPUT_DIMENSION, 16, gt)  #batchsize in 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "band=x_train.shape[-1]\n",
    "patch=x_train.shape[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = patch,\n",
    "    image_band = band,\n",
    "    num_classes = CLASSES_NUM,\n",
    "    dim = 64,\n",
    "    depth = 5,\n",
    "    heads = 4,\n",
    "    mlp_dim = 8,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    mode = 'ADCF'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 146, 64])\n",
      "5\n",
      "torch.Size([2, 170, 64])\n",
      "5\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 145, 64]          10,880\n",
      "           Dropout-2              [-1, 146, 64]               0\n",
      "         LayerNorm-3              [-1, 146, 64]             128\n",
      "            Linear-4             [-1, 146, 192]          12,288\n",
      "            Linear-5              [-1, 146, 64]           4,160\n",
      "           Dropout-6              [-1, 146, 64]               0\n",
      "         Attention-7              [-1, 146, 64]               0\n",
      "           PreNorm-8              [-1, 146, 64]               0\n",
      "          Residual-9              [-1, 146, 64]               0\n",
      "        LayerNorm-10              [-1, 146, 64]             128\n",
      "           Linear-11               [-1, 146, 8]             520\n",
      "             GELU-12               [-1, 146, 8]               0\n",
      "          Dropout-13               [-1, 146, 8]               0\n",
      "           Linear-14              [-1, 146, 64]             576\n",
      "          Dropout-15              [-1, 146, 64]               0\n",
      "      FeedForward-16              [-1, 146, 64]               0\n",
      "          PreNorm-17              [-1, 146, 64]               0\n",
      "         Residual-18              [-1, 146, 64]               0\n",
      "        LayerNorm-19              [-1, 146, 64]             128\n",
      "           Linear-20             [-1, 146, 192]          12,288\n",
      "           Linear-21              [-1, 146, 64]           4,160\n",
      "          Dropout-22              [-1, 146, 64]               0\n",
      "        Attention-23              [-1, 146, 64]               0\n",
      "          PreNorm-24              [-1, 146, 64]               0\n",
      "         Residual-25              [-1, 146, 64]               0\n",
      "        LayerNorm-26              [-1, 146, 64]             128\n",
      "           Linear-27               [-1, 146, 8]             520\n",
      "             GELU-28               [-1, 146, 8]               0\n",
      "          Dropout-29               [-1, 146, 8]               0\n",
      "           Linear-30              [-1, 146, 64]             576\n",
      "          Dropout-31              [-1, 146, 64]               0\n",
      "      FeedForward-32              [-1, 146, 64]               0\n",
      "          PreNorm-33              [-1, 146, 64]               0\n",
      "         Residual-34              [-1, 146, 64]               0\n",
      "           Conv2d-35           [-1, 146, 64, 1]          42,778\n",
      "        LayerNorm-36              [-1, 146, 64]             128\n",
      "           Linear-37             [-1, 146, 192]          12,288\n",
      "           Linear-38              [-1, 146, 64]           4,160\n",
      "          Dropout-39              [-1, 146, 64]               0\n",
      "        Attention-40              [-1, 146, 64]               0\n",
      "          PreNorm-41              [-1, 146, 64]               0\n",
      "         Residual-42              [-1, 146, 64]               0\n",
      "        LayerNorm-43              [-1, 146, 64]             128\n",
      "           Linear-44               [-1, 146, 8]             520\n",
      "             GELU-45               [-1, 146, 8]               0\n",
      "          Dropout-46               [-1, 146, 8]               0\n",
      "           Linear-47              [-1, 146, 64]             576\n",
      "          Dropout-48              [-1, 146, 64]               0\n",
      "      FeedForward-49              [-1, 146, 64]               0\n",
      "          PreNorm-50              [-1, 146, 64]               0\n",
      "         Residual-51              [-1, 146, 64]               0\n",
      "           Conv2d-52           [-1, 146, 64, 1]          42,778\n",
      "        LayerNorm-53              [-1, 146, 64]             128\n",
      "           Linear-54             [-1, 146, 192]          12,288\n",
      "           Linear-55              [-1, 146, 64]           4,160\n",
      "          Dropout-56              [-1, 146, 64]               0\n",
      "        Attention-57              [-1, 146, 64]               0\n",
      "          PreNorm-58              [-1, 146, 64]               0\n",
      "         Residual-59              [-1, 146, 64]               0\n",
      "        LayerNorm-60              [-1, 146, 64]             128\n",
      "           Linear-61               [-1, 146, 8]             520\n",
      "             GELU-62               [-1, 146, 8]               0\n",
      "          Dropout-63               [-1, 146, 8]               0\n",
      "           Linear-64              [-1, 146, 64]             576\n",
      "          Dropout-65              [-1, 146, 64]               0\n",
      "      FeedForward-66              [-1, 146, 64]               0\n",
      "          PreNorm-67              [-1, 146, 64]               0\n",
      "         Residual-68              [-1, 146, 64]               0\n",
      "           Conv2d-69           [-1, 146, 64, 1]          64,094\n",
      "        LayerNorm-70              [-1, 146, 64]             128\n",
      "           Linear-71             [-1, 146, 192]          12,288\n",
      "           Linear-72              [-1, 146, 64]           4,160\n",
      "          Dropout-73              [-1, 146, 64]               0\n",
      "        Attention-74              [-1, 146, 64]               0\n",
      "          PreNorm-75              [-1, 146, 64]               0\n",
      "         Residual-76              [-1, 146, 64]               0\n",
      "        LayerNorm-77              [-1, 146, 64]             128\n",
      "           Linear-78               [-1, 146, 8]             520\n",
      "             GELU-79               [-1, 146, 8]               0\n",
      "          Dropout-80               [-1, 146, 8]               0\n",
      "           Linear-81              [-1, 146, 64]             576\n",
      "          Dropout-82              [-1, 146, 64]               0\n",
      "      FeedForward-83              [-1, 146, 64]               0\n",
      "          PreNorm-84              [-1, 146, 64]               0\n",
      "         Residual-85              [-1, 146, 64]               0\n",
      "      Transformer-86              [-1, 146, 64]               0\n",
      "         Identity-87                   [-1, 64]               0\n",
      "           Linear-88              [-1, 169, 64]           9,344\n",
      "          Dropout-89              [-1, 170, 64]               0\n",
      "        LayerNorm-90              [-1, 170, 64]             128\n",
      "           Linear-91             [-1, 170, 192]          12,288\n",
      "           Linear-92              [-1, 170, 64]           4,160\n",
      "          Dropout-93              [-1, 170, 64]               0\n",
      "        Attention-94              [-1, 170, 64]               0\n",
      "          PreNorm-95              [-1, 170, 64]               0\n",
      "         Residual-96              [-1, 170, 64]               0\n",
      "        LayerNorm-97              [-1, 170, 64]             128\n",
      "           Linear-98               [-1, 170, 8]             520\n",
      "             GELU-99               [-1, 170, 8]               0\n",
      "         Dropout-100               [-1, 170, 8]               0\n",
      "          Linear-101              [-1, 170, 64]             576\n",
      "         Dropout-102              [-1, 170, 64]               0\n",
      "     FeedForward-103              [-1, 170, 64]               0\n",
      "         PreNorm-104              [-1, 170, 64]               0\n",
      "        Residual-105              [-1, 170, 64]               0\n",
      "       LayerNorm-106              [-1, 170, 64]             128\n",
      "          Linear-107             [-1, 170, 192]          12,288\n",
      "          Linear-108              [-1, 170, 64]           4,160\n",
      "         Dropout-109              [-1, 170, 64]               0\n",
      "       Attention-110              [-1, 170, 64]               0\n",
      "         PreNorm-111              [-1, 170, 64]               0\n",
      "        Residual-112              [-1, 170, 64]               0\n",
      "       LayerNorm-113              [-1, 170, 64]             128\n",
      "          Linear-114               [-1, 170, 8]             520\n",
      "            GELU-115               [-1, 170, 8]               0\n",
      "         Dropout-116               [-1, 170, 8]               0\n",
      "          Linear-117              [-1, 170, 64]             576\n",
      "         Dropout-118              [-1, 170, 64]               0\n",
      "     FeedForward-119              [-1, 170, 64]               0\n",
      "         PreNorm-120              [-1, 170, 64]               0\n",
      "        Residual-121              [-1, 170, 64]               0\n",
      "          Conv2d-122           [-1, 170, 64, 1]          57,970\n",
      "       LayerNorm-123              [-1, 170, 64]             128\n",
      "          Linear-124             [-1, 170, 192]          12,288\n",
      "          Linear-125              [-1, 170, 64]           4,160\n",
      "         Dropout-126              [-1, 170, 64]               0\n",
      "       Attention-127              [-1, 170, 64]               0\n",
      "         PreNorm-128              [-1, 170, 64]               0\n",
      "        Residual-129              [-1, 170, 64]               0\n",
      "       LayerNorm-130              [-1, 170, 64]             128\n",
      "          Linear-131               [-1, 170, 8]             520\n",
      "            GELU-132               [-1, 170, 8]               0\n",
      "         Dropout-133               [-1, 170, 8]               0\n",
      "          Linear-134              [-1, 170, 64]             576\n",
      "         Dropout-135              [-1, 170, 64]               0\n",
      "     FeedForward-136              [-1, 170, 64]               0\n",
      "         PreNorm-137              [-1, 170, 64]               0\n",
      "        Residual-138              [-1, 170, 64]               0\n",
      "          Conv2d-139           [-1, 170, 64, 1]          57,970\n",
      "       LayerNorm-140              [-1, 170, 64]             128\n",
      "          Linear-141             [-1, 170, 192]          12,288\n",
      "          Linear-142              [-1, 170, 64]           4,160\n",
      "         Dropout-143              [-1, 170, 64]               0\n",
      "       Attention-144              [-1, 170, 64]               0\n",
      "         PreNorm-145              [-1, 170, 64]               0\n",
      "        Residual-146              [-1, 170, 64]               0\n",
      "       LayerNorm-147              [-1, 170, 64]             128\n",
      "          Linear-148               [-1, 170, 8]             520\n",
      "            GELU-149               [-1, 170, 8]               0\n",
      "         Dropout-150               [-1, 170, 8]               0\n",
      "          Linear-151              [-1, 170, 64]             576\n",
      "         Dropout-152              [-1, 170, 64]               0\n",
      "     FeedForward-153              [-1, 170, 64]               0\n",
      "         PreNorm-154              [-1, 170, 64]               0\n",
      "        Residual-155              [-1, 170, 64]               0\n",
      "          Conv2d-156           [-1, 170, 64, 1]          86,870\n",
      "       LayerNorm-157              [-1, 170, 64]             128\n",
      "          Linear-158             [-1, 170, 192]          12,288\n",
      "          Linear-159              [-1, 170, 64]           4,160\n",
      "         Dropout-160              [-1, 170, 64]               0\n",
      "       Attention-161              [-1, 170, 64]               0\n",
      "         PreNorm-162              [-1, 170, 64]               0\n",
      "        Residual-163              [-1, 170, 64]               0\n",
      "       LayerNorm-164              [-1, 170, 64]             128\n",
      "          Linear-165               [-1, 170, 8]             520\n",
      "            GELU-166               [-1, 170, 8]               0\n",
      "         Dropout-167               [-1, 170, 8]               0\n",
      "          Linear-168              [-1, 170, 64]             576\n",
      "         Dropout-169              [-1, 170, 64]               0\n",
      "     FeedForward-170              [-1, 170, 64]               0\n",
      "         PreNorm-171              [-1, 170, 64]               0\n",
      "        Residual-172              [-1, 170, 64]               0\n",
      "     Transformer-173              [-1, 170, 64]               0\n",
      "        Identity-174                   [-1, 64]               0\n",
      "       LayerNorm-175                  [-1, 128]             256\n",
      "          Linear-176                   [-1, 14]           1,806\n",
      "================================================================\n",
      "Total params: 552,746\n",
      "Trainable params: 552,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 12.79\n",
      "Params size (MB): 2.11\n",
      "Estimated Total Size (MB): 14.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(patch,patch,band))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,\n",
    "          train_iter,\n",
    "          valida_iter,\n",
    "          loss,\n",
    "          optimizer,\n",
    "          device,\n",
    "          epochs,\n",
    "          early_stopping=True,\n",
    "          early_num=20):\n",
    "    loss_list = [100]\n",
    "    early_epoch = 0\n",
    "\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    start = time.time()\n",
    "    train_loss_list = []\n",
    "    valida_loss_list = []\n",
    "    train_acc_list = []\n",
    "    valida_acc_list = []\n",
    "    for epoch in range(epochs):\n",
    "        train_acc_sum, n = 0.0, 0\n",
    "        time_epoch = time.time()\n",
    "        lr_adjust = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=PARAM_EPOCH//10, gamma=0.9)\n",
    "        for X, y in train_iter:\n",
    "\n",
    "            batch_count, train_l_sum = 0, 0\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        lr_adjust.step()\n",
    "        valida_acc, valida_loss = record.evaluate_accuracy(\n",
    "            valida_iter, net, loss, device)\n",
    "        loss_list.append(valida_loss)\n",
    "\n",
    "        train_loss_list.append(train_l_sum)  # / batch_count)\n",
    "        train_acc_list.append(train_acc_sum / n)\n",
    "        valida_loss_list.append(valida_loss)\n",
    "        valida_acc_list.append(valida_acc)\n",
    "\n",
    "        print(\n",
    "            'epoch %d, train loss %.6f, train acc %.3f, valida loss %.6f, valida acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "               valida_loss, valida_acc, time.time() - time_epoch))\n",
    "\n",
    "        PATH = \"./net_DBA.pt\"\n",
    "\n",
    "        if early_stopping and loss_list[-2] < loss_list[-1]:\n",
    "            if early_epoch == 0:\n",
    "                torch.save(net.state_dict(), PATH)\n",
    "            early_epoch += 1\n",
    "            loss_list[-1] = loss_list[-2]\n",
    "            if early_epoch == early_num:\n",
    "                net.load_state_dict(torch.load(PATH))\n",
    "                break\n",
    "        else:\n",
    "            early_epoch = 0\n",
    "\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, time %.1f sec'\n",
    "          % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "             time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = PARAM_ITER\n",
    "KAPPA = []\n",
    "OA = []\n",
    "AA = []\n",
    "TRAINING_TIME = []\n",
    "TESTING_TIME = []\n",
    "ELEMENT_ACC = np.zeros((ITER, CLASSES_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_hsi,data,data_\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "-----Selecting Small Pieces from the Original Cube Data-----\n",
      "Train size:  (156, 13, 13, 145)\n",
      "Test size:  (2936, 13, 13, 145)\n",
      "Validation size:  (156, 13, 13, 145)\n",
      "training on  cuda\n",
      "epoch 1, train loss 2.745400, train acc 0.071, valida loss 2.539768, valida acc 0.115, time 5.1 sec\n",
      "epoch 2, train loss 2.446501, train acc 0.154, valida loss 2.255063, valida acc 0.224, time 0.2 sec\n",
      "epoch 3, train loss 2.153551, train acc 0.359, valida loss 2.108610, valida acc 0.372, time 0.2 sec\n",
      "epoch 4, train loss 2.108338, train acc 0.506, valida loss 1.921215, valida acc 0.462, time 0.2 sec\n",
      "epoch 5, train loss 1.668115, train acc 0.647, valida loss 1.667006, valida acc 0.545, time 0.2 sec\n",
      "epoch 6, train loss 1.550220, train acc 0.763, valida loss 1.412457, valida acc 0.667, time 0.2 sec\n",
      "epoch 7, train loss 1.058279, train acc 0.885, valida loss 1.163644, valida acc 0.769, time 0.2 sec\n",
      "epoch 8, train loss 0.981452, train acc 0.897, valida loss 0.958120, valida acc 0.840, time 0.2 sec\n",
      "epoch 9, train loss 0.840908, train acc 0.962, valida loss 0.821750, valida acc 0.897, time 0.2 sec\n",
      "epoch 10, train loss 0.614186, train acc 0.981, valida loss 0.726142, valida acc 0.917, time 0.2 sec\n",
      "epoch 11, train loss 0.520247, train acc 0.994, valida loss 0.626115, valida acc 0.936, time 0.2 sec\n",
      "epoch 12, train loss 0.399139, train acc 0.994, valida loss 0.514169, valida acc 0.923, time 0.2 sec\n",
      "epoch 13, train loss 0.372364, train acc 1.000, valida loss 0.431121, valida acc 0.923, time 0.2 sec\n",
      "epoch 14, train loss 0.307881, train acc 1.000, valida loss 0.374965, valida acc 0.929, time 0.2 sec\n",
      "epoch 15, train loss 0.192077, train acc 1.000, valida loss 0.337807, valida acc 0.949, time 0.2 sec\n",
      "epoch 16, train loss 0.225100, train acc 1.000, valida loss 0.304851, valida acc 0.955, time 0.2 sec\n",
      "epoch 17, train loss 0.163088, train acc 1.000, valida loss 0.267876, valida acc 0.955, time 0.2 sec\n",
      "epoch 18, train loss 0.191483, train acc 1.000, valida loss 0.234436, valida acc 0.955, time 0.2 sec\n",
      "epoch 19, train loss 0.124675, train acc 1.000, valida loss 0.206590, valida acc 0.955, time 0.2 sec\n",
      "epoch 20, train loss 0.129736, train acc 1.000, valida loss 0.185138, valida acc 0.955, time 0.2 sec\n",
      "epoch 21, train loss 0.099593, train acc 1.000, valida loss 0.166460, valida acc 0.955, time 0.2 sec\n",
      "epoch 22, train loss 0.105698, train acc 1.000, valida loss 0.151329, valida acc 0.962, time 0.2 sec\n",
      "epoch 23, train loss 0.086158, train acc 1.000, valida loss 0.139383, valida acc 0.962, time 0.2 sec\n",
      "epoch 24, train loss 0.065706, train acc 1.000, valida loss 0.130187, valida acc 0.962, time 0.2 sec\n",
      "epoch 25, train loss 0.066359, train acc 1.000, valida loss 0.121339, valida acc 0.962, time 0.2 sec\n",
      "epoch 26, train loss 0.057159, train acc 1.000, valida loss 0.111773, valida acc 0.962, time 0.2 sec\n",
      "epoch 27, train loss 0.054662, train acc 1.000, valida loss 0.103421, valida acc 0.962, time 0.2 sec\n",
      "epoch 28, train loss 0.047358, train acc 1.000, valida loss 0.096128, valida acc 0.962, time 0.2 sec\n",
      "epoch 29, train loss 0.050393, train acc 1.000, valida loss 0.090022, valida acc 0.962, time 0.2 sec\n",
      "epoch 30, train loss 0.044425, train acc 1.000, valida loss 0.084156, valida acc 0.962, time 0.2 sec\n",
      "epoch 31, train loss 0.040079, train acc 1.000, valida loss 0.080037, valida acc 0.962, time 0.2 sec\n",
      "epoch 32, train loss 0.036175, train acc 1.000, valida loss 0.076617, valida acc 0.962, time 0.2 sec\n",
      "epoch 33, train loss 0.040596, train acc 1.000, valida loss 0.073522, valida acc 0.962, time 0.2 sec\n",
      "epoch 34, train loss 0.036292, train acc 1.000, valida loss 0.070591, valida acc 0.962, time 0.2 sec\n",
      "epoch 35, train loss 0.028144, train acc 1.000, valida loss 0.068036, valida acc 0.962, time 0.2 sec\n",
      "epoch 36, train loss 0.031313, train acc 1.000, valida loss 0.065453, valida acc 0.962, time 0.2 sec\n",
      "epoch 37, train loss 0.029659, train acc 1.000, valida loss 0.062884, valida acc 0.962, time 0.2 sec\n",
      "epoch 38, train loss 0.030268, train acc 1.000, valida loss 0.060631, valida acc 0.962, time 0.2 sec\n",
      "epoch 39, train loss 0.024849, train acc 1.000, valida loss 0.059082, valida acc 0.962, time 0.2 sec\n",
      "epoch 40, train loss 0.023160, train acc 1.000, valida loss 0.057453, valida acc 0.962, time 0.2 sec\n",
      "epoch 41, train loss 0.023698, train acc 1.000, valida loss 0.055816, valida acc 0.962, time 0.2 sec\n",
      "epoch 42, train loss 0.021959, train acc 1.000, valida loss 0.053958, valida acc 0.962, time 0.2 sec\n",
      "epoch 43, train loss 0.022734, train acc 1.000, valida loss 0.052565, valida acc 0.962, time 0.2 sec\n",
      "epoch 44, train loss 0.020549, train acc 1.000, valida loss 0.051259, valida acc 0.962, time 0.2 sec\n",
      "epoch 45, train loss 0.021791, train acc 1.000, valida loss 0.050078, valida acc 0.962, time 0.2 sec\n",
      "epoch 46, train loss 0.019748, train acc 1.000, valida loss 0.048789, valida acc 0.962, time 0.2 sec\n",
      "epoch 47, train loss 0.018168, train acc 1.000, valida loss 0.047442, valida acc 0.962, time 0.2 sec\n",
      "epoch 48, train loss 0.017167, train acc 1.000, valida loss 0.046127, valida acc 0.962, time 0.2 sec\n",
      "epoch 49, train loss 0.017116, train acc 1.000, valida loss 0.045043, valida acc 0.962, time 0.2 sec\n",
      "epoch 50, train loss 0.018636, train acc 1.000, valida loss 0.044165, valida acc 0.962, time 0.2 sec\n",
      "epoch 51, train loss 0.015469, train acc 1.000, valida loss 0.042994, valida acc 0.962, time 0.2 sec\n",
      "epoch 52, train loss 0.016118, train acc 1.000, valida loss 0.041942, valida acc 0.962, time 0.2 sec\n",
      "epoch 53, train loss 0.015535, train acc 1.000, valida loss 0.040864, valida acc 0.962, time 0.2 sec\n",
      "epoch 54, train loss 0.014850, train acc 1.000, valida loss 0.040106, valida acc 0.962, time 0.2 sec\n",
      "epoch 55, train loss 0.014164, train acc 1.000, valida loss 0.039502, valida acc 0.962, time 0.2 sec\n",
      "epoch 56, train loss 0.013356, train acc 1.000, valida loss 0.039097, valida acc 0.962, time 0.2 sec\n",
      "epoch 57, train loss 0.012977, train acc 1.000, valida loss 0.038873, valida acc 0.962, time 0.2 sec\n",
      "epoch 58, train loss 0.013505, train acc 1.000, valida loss 0.038535, valida acc 0.962, time 0.2 sec\n",
      "epoch 59, train loss 0.012658, train acc 1.000, valida loss 0.038100, valida acc 0.962, time 0.2 sec\n",
      "epoch 60, train loss 0.011892, train acc 1.000, valida loss 0.037430, valida acc 0.962, time 0.2 sec\n",
      "epoch 61, train loss 0.012184, train acc 1.000, valida loss 0.037051, valida acc 0.962, time 0.2 sec\n",
      "epoch 62, train loss 0.011898, train acc 1.000, valida loss 0.036527, valida acc 0.962, time 0.2 sec\n",
      "epoch 63, train loss 0.011580, train acc 1.000, valida loss 0.035861, valida acc 0.962, time 0.2 sec\n",
      "epoch 64, train loss 0.011593, train acc 1.000, valida loss 0.035103, valida acc 0.962, time 0.2 sec\n",
      "epoch 65, train loss 0.010977, train acc 1.000, valida loss 0.034118, valida acc 0.962, time 0.2 sec\n",
      "epoch 66, train loss 0.010260, train acc 1.000, valida loss 0.033387, valida acc 0.962, time 0.2 sec\n",
      "epoch 67, train loss 0.009476, train acc 1.000, valida loss 0.032914, valida acc 0.962, time 0.2 sec\n",
      "epoch 68, train loss 0.011021, train acc 1.000, valida loss 0.032668, valida acc 0.962, time 0.2 sec\n",
      "epoch 69, train loss 0.010154, train acc 1.000, valida loss 0.032343, valida acc 0.962, time 0.2 sec\n",
      "epoch 70, train loss 0.008939, train acc 1.000, valida loss 0.031587, valida acc 0.962, time 0.2 sec\n",
      "epoch 71, train loss 0.009576, train acc 1.000, valida loss 0.030631, valida acc 0.962, time 0.2 sec\n",
      "epoch 72, train loss 0.009861, train acc 1.000, valida loss 0.029996, valida acc 0.962, time 0.2 sec\n",
      "epoch 73, train loss 0.008835, train acc 1.000, valida loss 0.029293, valida acc 0.962, time 0.2 sec\n",
      "epoch 74, train loss 0.009428, train acc 1.000, valida loss 0.028725, valida acc 0.962, time 0.2 sec\n",
      "epoch 75, train loss 0.007925, train acc 1.000, valida loss 0.028323, valida acc 0.962, time 0.2 sec\n",
      "epoch 76, train loss 0.008106, train acc 1.000, valida loss 0.027976, valida acc 0.962, time 0.2 sec\n",
      "epoch 77, train loss 0.008038, train acc 1.000, valida loss 0.027790, valida acc 0.962, time 0.2 sec\n",
      "epoch 78, train loss 0.007921, train acc 1.000, valida loss 0.027451, valida acc 0.962, time 0.2 sec\n",
      "epoch 79, train loss 0.007517, train acc 1.000, valida loss 0.026745, valida acc 0.962, time 0.2 sec\n",
      "epoch 80, train loss 0.008027, train acc 1.000, valida loss 0.026054, valida acc 0.962, time 0.2 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81, train loss 0.008117, train acc 1.000, valida loss 0.025347, valida acc 0.962, time 0.2 sec\n",
      "epoch 82, train loss 0.007487, train acc 1.000, valida loss 0.024856, valida acc 0.962, time 0.2 sec\n",
      "epoch 83, train loss 0.006951, train acc 1.000, valida loss 0.024612, valida acc 0.962, time 0.2 sec\n",
      "epoch 84, train loss 0.006363, train acc 1.000, valida loss 0.024605, valida acc 0.962, time 0.2 sec\n",
      "epoch 85, train loss 0.007008, train acc 1.000, valida loss 0.024377, valida acc 0.962, time 0.2 sec\n",
      "epoch 86, train loss 0.007004, train acc 1.000, valida loss 0.023911, valida acc 0.962, time 0.2 sec\n",
      "epoch 87, train loss 0.006198, train acc 1.000, valida loss 0.023533, valida acc 0.962, time 0.2 sec\n",
      "epoch 88, train loss 0.007020, train acc 1.000, valida loss 0.023031, valida acc 0.962, time 0.2 sec\n",
      "epoch 89, train loss 0.006233, train acc 1.000, valida loss 0.022464, valida acc 0.962, time 0.2 sec\n",
      "epoch 90, train loss 0.006509, train acc 1.000, valida loss 0.021973, valida acc 0.962, time 0.2 sec\n",
      "epoch 91, train loss 0.006081, train acc 1.000, valida loss 0.021576, valida acc 0.962, time 0.2 sec\n",
      "epoch 92, train loss 0.006671, train acc 1.000, valida loss 0.021167, valida acc 0.962, time 0.2 sec\n",
      "epoch 93, train loss 0.006031, train acc 1.000, valida loss 0.020583, valida acc 0.962, time 0.2 sec\n",
      "epoch 94, train loss 0.005803, train acc 1.000, valida loss 0.020102, valida acc 0.962, time 0.2 sec\n",
      "epoch 95, train loss 0.005544, train acc 1.000, valida loss 0.019815, valida acc 0.962, time 0.2 sec\n",
      "epoch 96, train loss 0.005652, train acc 1.000, valida loss 0.019694, valida acc 0.962, time 0.2 sec\n",
      "epoch 97, train loss 0.005672, train acc 1.000, valida loss 0.019396, valida acc 0.962, time 0.2 sec\n",
      "epoch 98, train loss 0.005991, train acc 1.000, valida loss 0.019016, valida acc 0.962, time 0.2 sec\n",
      "epoch 99, train loss 0.004934, train acc 1.000, valida loss 0.018634, valida acc 0.962, time 0.2 sec\n",
      "epoch 100, train loss 0.005315, train acc 1.000, valida loss 0.018449, valida acc 0.962, time 0.2 sec\n",
      "epoch 100, loss 0.0053, train acc 1.000, time 26.5 sec\n",
      "iter: 1\n",
      "-----Selecting Small Pieces from the Original Cube Data-----\n",
      "Train size:  (156, 13, 13, 145)\n",
      "Test size:  (2936, 13, 13, 145)\n",
      "Validation size:  (156, 13, 13, 145)\n",
      "training on  cuda\n",
      "epoch 1, train loss 2.600232, train acc 0.096, valida loss 2.700199, valida acc 0.179, time 0.2 sec\n",
      "epoch 2, train loss 2.579538, train acc 0.160, valida loss 2.595587, valida acc 0.218, time 0.2 sec\n",
      "epoch 3, train loss 2.254116, train acc 0.314, valida loss 2.429169, valida acc 0.340, time 0.2 sec\n",
      "epoch 4, train loss 1.930080, train acc 0.455, valida loss 2.232844, valida acc 0.455, time 0.2 sec\n",
      "epoch 5, train loss 1.876596, train acc 0.609, valida loss 1.990942, valida acc 0.551, time 0.2 sec\n",
      "epoch 6, train loss 1.425902, train acc 0.667, valida loss 1.754603, valida acc 0.705, time 0.2 sec\n",
      "epoch 7, train loss 1.130269, train acc 0.859, valida loss 1.526240, valida acc 0.808, time 0.2 sec\n",
      "epoch 8, train loss 1.001832, train acc 0.891, valida loss 1.335615, valida acc 0.833, time 0.2 sec\n",
      "epoch 9, train loss 0.915543, train acc 0.929, valida loss 1.169302, valida acc 0.846, time 0.2 sec\n",
      "epoch 10, train loss 0.611670, train acc 0.955, valida loss 1.020639, valida acc 0.865, time 0.2 sec\n",
      "epoch 11, train loss 0.651831, train acc 0.962, valida loss 0.886199, valida acc 0.910, time 0.2 sec\n",
      "epoch 12, train loss 0.511270, train acc 0.987, valida loss 0.759426, valida acc 0.929, time 0.2 sec\n",
      "epoch 13, train loss 0.381037, train acc 1.000, valida loss 0.670078, valida acc 0.936, time 0.2 sec\n",
      "epoch 14, train loss 0.345825, train acc 0.994, valida loss 0.599406, valida acc 0.936, time 0.2 sec\n",
      "epoch 15, train loss 0.316550, train acc 0.994, valida loss 0.547559, valida acc 0.929, time 0.2 sec\n",
      "epoch 16, train loss 0.246534, train acc 1.000, valida loss 0.511269, valida acc 0.929, time 0.2 sec\n",
      "epoch 17, train loss 0.204794, train acc 1.000, valida loss 0.488313, valida acc 0.955, time 0.2 sec\n",
      "epoch 18, train loss 0.178143, train acc 1.000, valida loss 0.465590, valida acc 0.955, time 0.2 sec\n",
      "epoch 19, train loss 0.159152, train acc 1.000, valida loss 0.439442, valida acc 0.955, time 0.2 sec\n",
      "epoch 20, train loss 0.132422, train acc 1.000, valida loss 0.410373, valida acc 0.955, time 0.2 sec\n",
      "epoch 21, train loss 0.133873, train acc 1.000, valida loss 0.386460, valida acc 0.955, time 0.2 sec\n",
      "epoch 22, train loss 0.119582, train acc 1.000, valida loss 0.369794, valida acc 0.955, time 0.2 sec\n",
      "epoch 23, train loss 0.100380, train acc 1.000, valida loss 0.356396, valida acc 0.955, time 0.2 sec\n",
      "epoch 24, train loss 0.089579, train acc 1.000, valida loss 0.342869, valida acc 0.962, time 0.2 sec\n",
      "epoch 25, train loss 0.081914, train acc 1.000, valida loss 0.331039, valida acc 0.962, time 0.2 sec\n",
      "epoch 26, train loss 0.061596, train acc 1.000, valida loss 0.321210, valida acc 0.962, time 0.2 sec\n",
      "epoch 27, train loss 0.068273, train acc 1.000, valida loss 0.310883, valida acc 0.962, time 0.2 sec\n",
      "epoch 28, train loss 0.060689, train acc 1.000, valida loss 0.302856, valida acc 0.962, time 0.2 sec\n",
      "epoch 29, train loss 0.056510, train acc 1.000, valida loss 0.295464, valida acc 0.962, time 0.2 sec\n",
      "epoch 30, train loss 0.055149, train acc 1.000, valida loss 0.290376, valida acc 0.962, time 0.2 sec\n",
      "epoch 31, train loss 0.049691, train acc 1.000, valida loss 0.285893, valida acc 0.962, time 0.2 sec\n",
      "epoch 32, train loss 0.047145, train acc 1.000, valida loss 0.282089, valida acc 0.962, time 0.2 sec\n",
      "epoch 33, train loss 0.041543, train acc 1.000, valida loss 0.278903, valida acc 0.962, time 0.2 sec\n",
      "epoch 34, train loss 0.038431, train acc 1.000, valida loss 0.276547, valida acc 0.962, time 0.2 sec\n",
      "epoch 35, train loss 0.034591, train acc 1.000, valida loss 0.272639, valida acc 0.962, time 0.2 sec\n",
      "epoch 36, train loss 0.039583, train acc 1.000, valida loss 0.269902, valida acc 0.962, time 0.2 sec\n",
      "epoch 37, train loss 0.037244, train acc 1.000, valida loss 0.266876, valida acc 0.962, time 0.2 sec\n",
      "epoch 38, train loss 0.034061, train acc 1.000, valida loss 0.264197, valida acc 0.962, time 0.2 sec\n",
      "epoch 39, train loss 0.028819, train acc 1.000, valida loss 0.261676, valida acc 0.962, time 0.2 sec\n",
      "epoch 40, train loss 0.028574, train acc 1.000, valida loss 0.258556, valida acc 0.962, time 0.2 sec\n",
      "epoch 41, train loss 0.028452, train acc 1.000, valida loss 0.256621, valida acc 0.962, time 0.2 sec\n",
      "epoch 42, train loss 0.026058, train acc 1.000, valida loss 0.255336, valida acc 0.962, time 0.2 sec\n",
      "epoch 43, train loss 0.024783, train acc 1.000, valida loss 0.254984, valida acc 0.968, time 0.2 sec\n",
      "epoch 44, train loss 0.023892, train acc 1.000, valida loss 0.254366, valida acc 0.968, time 0.2 sec\n",
      "epoch 45, train loss 0.025471, train acc 1.000, valida loss 0.253456, valida acc 0.962, time 0.2 sec\n",
      "epoch 46, train loss 0.023947, train acc 1.000, valida loss 0.252741, valida acc 0.968, time 0.2 sec\n",
      "epoch 47, train loss 0.021091, train acc 1.000, valida loss 0.253631, valida acc 0.968, time 0.2 sec\n",
      "epoch 48, train loss 0.020713, train acc 1.000, valida loss 0.254255, valida acc 0.968, time 0.2 sec\n",
      "epoch 49, train loss 0.019460, train acc 1.000, valida loss 0.253526, valida acc 0.968, time 0.2 sec\n",
      "epoch 50, train loss 0.019285, train acc 1.000, valida loss 0.253669, valida acc 0.968, time 0.2 sec\n",
      "epoch 51, train loss 0.018272, train acc 1.000, valida loss 0.253991, valida acc 0.974, time 0.2 sec\n",
      "epoch 52, train loss 0.019443, train acc 1.000, valida loss 0.254060, valida acc 0.974, time 0.2 sec\n",
      "epoch 53, train loss 0.017514, train acc 1.000, valida loss 0.254454, valida acc 0.974, time 0.2 sec\n",
      "epoch 54, train loss 0.017159, train acc 1.000, valida loss 0.253900, valida acc 0.974, time 0.2 sec\n",
      "epoch 55, train loss 0.014744, train acc 1.000, valida loss 0.251813, valida acc 0.974, time 0.2 sec\n",
      "epoch 56, train loss 0.015472, train acc 1.000, valida loss 0.250025, valida acc 0.974, time 0.2 sec\n",
      "epoch 57, train loss 0.015914, train acc 1.000, valida loss 0.249088, valida acc 0.974, time 0.2 sec\n",
      "epoch 58, train loss 0.014157, train acc 1.000, valida loss 0.249295, valida acc 0.974, time 0.2 sec\n",
      "epoch 59, train loss 0.013040, train acc 1.000, valida loss 0.248882, valida acc 0.974, time 0.2 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60, train loss 0.013283, train acc 1.000, valida loss 0.248565, valida acc 0.974, time 0.2 sec\n",
      "epoch 61, train loss 0.012887, train acc 1.000, valida loss 0.248476, valida acc 0.974, time 0.2 sec\n",
      "epoch 62, train loss 0.013007, train acc 1.000, valida loss 0.248582, valida acc 0.974, time 0.2 sec\n",
      "epoch 63, train loss 0.011158, train acc 1.000, valida loss 0.247742, valida acc 0.974, time 0.2 sec\n",
      "epoch 64, train loss 0.012305, train acc 1.000, valida loss 0.245695, valida acc 0.974, time 0.2 sec\n",
      "epoch 65, train loss 0.010785, train acc 1.000, valida loss 0.243887, valida acc 0.974, time 0.2 sec\n",
      "epoch 66, train loss 0.010974, train acc 1.000, valida loss 0.242969, valida acc 0.974, time 0.2 sec\n",
      "epoch 67, train loss 0.010325, train acc 1.000, valida loss 0.242840, valida acc 0.974, time 0.2 sec\n",
      "epoch 68, train loss 0.011879, train acc 1.000, valida loss 0.242619, valida acc 0.974, time 0.2 sec\n",
      "epoch 69, train loss 0.010038, train acc 1.000, valida loss 0.243636, valida acc 0.974, time 0.2 sec\n",
      "epoch 70, train loss 0.011109, train acc 1.000, valida loss 0.244488, valida acc 0.974, time 0.2 sec\n",
      "epoch 71, train loss 0.009200, train acc 1.000, valida loss 0.245147, valida acc 0.974, time 0.2 sec\n",
      "epoch 72, train loss 0.009707, train acc 1.000, valida loss 0.245867, valida acc 0.974, time 0.2 sec\n",
      "epoch 73, train loss 0.008729, train acc 1.000, valida loss 0.246526, valida acc 0.974, time 0.2 sec\n",
      "epoch 74, train loss 0.009041, train acc 1.000, valida loss 0.246022, valida acc 0.974, time 0.2 sec\n",
      "epoch 75, train loss 0.009472, train acc 1.000, valida loss 0.245668, valida acc 0.974, time 0.2 sec\n",
      "epoch 76, train loss 0.008948, train acc 1.000, valida loss 0.244314, valida acc 0.974, time 0.2 sec\n",
      "epoch 77, train loss 0.009055, train acc 1.000, valida loss 0.243547, valida acc 0.974, time 0.2 sec\n",
      "epoch 78, train loss 0.008304, train acc 1.000, valida loss 0.243249, valida acc 0.974, time 0.2 sec\n",
      "epoch 79, train loss 0.009538, train acc 1.000, valida loss 0.243466, valida acc 0.974, time 0.2 sec\n",
      "epoch 80, train loss 0.008316, train acc 1.000, valida loss 0.243228, valida acc 0.974, time 0.2 sec\n",
      "epoch 81, train loss 0.008300, train acc 1.000, valida loss 0.243672, valida acc 0.974, time 0.2 sec\n",
      "epoch 82, train loss 0.007620, train acc 1.000, valida loss 0.244447, valida acc 0.974, time 0.2 sec\n",
      "epoch 83, train loss 0.007850, train acc 1.000, valida loss 0.244693, valida acc 0.974, time 0.2 sec\n",
      "epoch 84, train loss 0.008847, train acc 1.000, valida loss 0.243332, valida acc 0.974, time 0.2 sec\n",
      "epoch 85, train loss 0.007725, train acc 1.000, valida loss 0.241022, valida acc 0.974, time 0.2 sec\n",
      "epoch 86, train loss 0.007979, train acc 1.000, valida loss 0.240749, valida acc 0.974, time 0.2 sec\n",
      "epoch 87, train loss 0.007555, train acc 1.000, valida loss 0.242137, valida acc 0.974, time 0.2 sec\n",
      "epoch 88, train loss 0.006654, train acc 1.000, valida loss 0.242642, valida acc 0.974, time 0.2 sec\n",
      "epoch 89, train loss 0.006745, train acc 1.000, valida loss 0.243205, valida acc 0.974, time 0.2 sec\n",
      "epoch 90, train loss 0.006597, train acc 1.000, valida loss 0.243925, valida acc 0.974, time 0.2 sec\n",
      "epoch 91, train loss 0.006648, train acc 1.000, valida loss 0.244151, valida acc 0.974, time 0.2 sec\n",
      "epoch 92, train loss 0.006607, train acc 1.000, valida loss 0.243788, valida acc 0.974, time 0.2 sec\n",
      "epoch 93, train loss 0.006492, train acc 1.000, valida loss 0.243553, valida acc 0.974, time 0.2 sec\n",
      "epoch 94, train loss 0.006443, train acc 1.000, valida loss 0.243043, valida acc 0.974, time 0.2 sec\n",
      "epoch 95, train loss 0.006313, train acc 1.000, valida loss 0.243042, valida acc 0.974, time 0.2 sec\n",
      "epoch 96, train loss 0.006672, train acc 1.000, valida loss 0.243221, valida acc 0.974, time 0.2 sec\n",
      "epoch 97, train loss 0.005886, train acc 1.000, valida loss 0.243693, valida acc 0.974, time 0.2 sec\n",
      "epoch 98, train loss 0.006707, train acc 1.000, valida loss 0.243566, valida acc 0.974, time 0.2 sec\n",
      "epoch 99, train loss 0.005409, train acc 1.000, valida loss 0.243704, valida acc 0.974, time 0.2 sec\n",
      "epoch 100, train loss 0.005714, train acc 1.000, valida loss 0.243178, valida acc 0.974, time 0.2 sec\n",
      "epoch 100, loss 0.0057, train acc 1.000, time 21.7 sec\n",
      "iter: 2\n",
      "-----Selecting Small Pieces from the Original Cube Data-----\n",
      "Train size:  (156, 13, 13, 145)\n",
      "Test size:  (2936, 13, 13, 145)\n",
      "Validation size:  (156, 13, 13, 145)\n",
      "training on  cuda\n",
      "epoch 1, train loss 2.784342, train acc 0.064, valida loss 2.517096, valida acc 0.154, time 0.2 sec\n",
      "epoch 2, train loss 2.510075, train acc 0.160, valida loss 2.442296, valida acc 0.179, time 0.2 sec\n",
      "epoch 3, train loss 2.410087, train acc 0.301, valida loss 2.322592, valida acc 0.295, time 0.2 sec\n",
      "epoch 4, train loss 1.996430, train acc 0.487, valida loss 2.142559, valida acc 0.404, time 0.2 sec\n",
      "epoch 5, train loss 1.834079, train acc 0.647, valida loss 1.920196, valida acc 0.545, time 0.2 sec\n",
      "epoch 6, train loss 1.655861, train acc 0.769, valida loss 1.657640, valida acc 0.635, time 0.2 sec\n",
      "epoch 7, train loss 1.316965, train acc 0.840, valida loss 1.380513, valida acc 0.801, time 0.2 sec\n",
      "epoch 8, train loss 1.013920, train acc 0.891, valida loss 1.135133, valida acc 0.859, time 0.2 sec\n",
      "epoch 9, train loss 0.786827, train acc 0.929, valida loss 0.946494, valida acc 0.859, time 0.2 sec\n",
      "epoch 10, train loss 0.872165, train acc 0.942, valida loss 0.786985, valida acc 0.878, time 0.2 sec\n",
      "epoch 11, train loss 0.569734, train acc 0.962, valida loss 0.671595, valida acc 0.904, time 0.2 sec\n",
      "epoch 12, train loss 0.496245, train acc 0.981, valida loss 0.571222, valida acc 0.904, time 0.2 sec\n",
      "epoch 13, train loss 0.428070, train acc 0.981, valida loss 0.501033, valida acc 0.923, time 0.2 sec\n",
      "epoch 14, train loss 0.312162, train acc 0.994, valida loss 0.448339, valida acc 0.929, time 0.2 sec\n",
      "epoch 15, train loss 0.281205, train acc 1.000, valida loss 0.401360, valida acc 0.942, time 0.2 sec\n",
      "epoch 16, train loss 0.218637, train acc 1.000, valida loss 0.361563, valida acc 0.968, time 0.2 sec\n",
      "epoch 17, train loss 0.187350, train acc 1.000, valida loss 0.319938, valida acc 0.974, time 0.2 sec\n",
      "epoch 18, train loss 0.172831, train acc 1.000, valida loss 0.277482, valida acc 0.981, time 0.2 sec\n",
      "epoch 19, train loss 0.152926, train acc 1.000, valida loss 0.239624, valida acc 0.981, time 0.2 sec\n",
      "epoch 20, train loss 0.140383, train acc 1.000, valida loss 0.214705, valida acc 0.981, time 0.2 sec\n",
      "epoch 21, train loss 0.102674, train acc 1.000, valida loss 0.197087, valida acc 0.981, time 0.2 sec\n",
      "epoch 22, train loss 0.123120, train acc 1.000, valida loss 0.186156, valida acc 0.981, time 0.2 sec\n",
      "epoch 23, train loss 0.095176, train acc 1.000, valida loss 0.180060, valida acc 0.994, time 0.2 sec\n",
      "epoch 24, train loss 0.078030, train acc 1.000, valida loss 0.171145, valida acc 0.994, time 0.2 sec\n",
      "epoch 25, train loss 0.090515, train acc 1.000, valida loss 0.155428, valida acc 0.994, time 0.2 sec\n",
      "epoch 26, train loss 0.063035, train acc 1.000, valida loss 0.134302, valida acc 1.000, time 0.2 sec\n",
      "epoch 27, train loss 0.054939, train acc 1.000, valida loss 0.117832, valida acc 1.000, time 0.2 sec\n",
      "epoch 28, train loss 0.056633, train acc 1.000, valida loss 0.107306, valida acc 1.000, time 0.2 sec\n",
      "epoch 29, train loss 0.046662, train acc 1.000, valida loss 0.100114, valida acc 1.000, time 0.2 sec\n",
      "epoch 30, train loss 0.048775, train acc 1.000, valida loss 0.094477, valida acc 1.000, time 0.2 sec\n",
      "epoch 31, train loss 0.043598, train acc 1.000, valida loss 0.090644, valida acc 1.000, time 0.2 sec\n",
      "epoch 32, train loss 0.042539, train acc 1.000, valida loss 0.085054, valida acc 1.000, time 0.2 sec\n",
      "epoch 33, train loss 0.038407, train acc 1.000, valida loss 0.079577, valida acc 1.000, time 0.2 sec\n",
      "epoch 34, train loss 0.040082, train acc 1.000, valida loss 0.074681, valida acc 1.000, time 0.2 sec\n",
      "epoch 35, train loss 0.032793, train acc 1.000, valida loss 0.072498, valida acc 1.000, time 0.2 sec\n",
      "epoch 36, train loss 0.030394, train acc 1.000, valida loss 0.069200, valida acc 1.000, time 0.2 sec\n",
      "epoch 37, train loss 0.030597, train acc 1.000, valida loss 0.065849, valida acc 1.000, time 0.2 sec\n",
      "epoch 38, train loss 0.026115, train acc 1.000, valida loss 0.062507, valida acc 1.000, time 0.2 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39, train loss 0.022953, train acc 1.000, valida loss 0.058985, valida acc 1.000, time 0.2 sec\n",
      "epoch 40, train loss 0.023050, train acc 1.000, valida loss 0.055955, valida acc 1.000, time 0.2 sec\n",
      "epoch 41, train loss 0.022965, train acc 1.000, valida loss 0.053283, valida acc 1.000, time 0.2 sec\n",
      "epoch 42, train loss 0.025926, train acc 1.000, valida loss 0.051384, valida acc 1.000, time 0.2 sec\n",
      "epoch 43, train loss 0.019789, train acc 1.000, valida loss 0.049333, valida acc 1.000, time 0.2 sec\n",
      "epoch 44, train loss 0.020970, train acc 1.000, valida loss 0.047793, valida acc 1.000, time 0.2 sec\n",
      "epoch 45, train loss 0.019963, train acc 1.000, valida loss 0.046288, valida acc 1.000, time 0.2 sec\n",
      "epoch 46, train loss 0.018021, train acc 1.000, valida loss 0.044757, valida acc 1.000, time 0.2 sec\n",
      "epoch 47, train loss 0.018296, train acc 1.000, valida loss 0.042912, valida acc 1.000, time 0.2 sec\n",
      "epoch 48, train loss 0.018538, train acc 1.000, valida loss 0.041219, valida acc 1.000, time 0.2 sec\n",
      "epoch 49, train loss 0.018017, train acc 1.000, valida loss 0.039733, valida acc 1.000, time 0.2 sec\n",
      "epoch 50, train loss 0.017037, train acc 1.000, valida loss 0.038342, valida acc 1.000, time 0.2 sec\n",
      "epoch 51, train loss 0.016612, train acc 1.000, valida loss 0.037142, valida acc 1.000, time 0.2 sec\n",
      "epoch 52, train loss 0.017657, train acc 1.000, valida loss 0.036158, valida acc 1.000, time 0.2 sec\n",
      "epoch 53, train loss 0.015659, train acc 1.000, valida loss 0.035504, valida acc 1.000, time 0.2 sec\n",
      "epoch 54, train loss 0.014824, train acc 1.000, valida loss 0.034667, valida acc 1.000, time 0.2 sec\n",
      "epoch 55, train loss 0.013918, train acc 1.000, valida loss 0.033629, valida acc 1.000, time 0.2 sec\n",
      "epoch 56, train loss 0.013626, train acc 1.000, valida loss 0.032784, valida acc 1.000, time 0.2 sec\n",
      "epoch 57, train loss 0.013515, train acc 1.000, valida loss 0.031850, valida acc 1.000, time 0.2 sec\n",
      "epoch 58, train loss 0.012916, train acc 1.000, valida loss 0.031035, valida acc 1.000, time 0.2 sec\n",
      "epoch 59, train loss 0.011861, train acc 1.000, valida loss 0.030437, valida acc 1.000, time 0.2 sec\n",
      "epoch 60, train loss 0.013646, train acc 1.000, valida loss 0.030023, valida acc 1.000, time 0.2 sec\n",
      "epoch 61, train loss 0.013161, train acc 1.000, valida loss 0.029569, valida acc 1.000, time 0.2 sec\n",
      "epoch 62, train loss 0.010920, train acc 1.000, valida loss 0.028915, valida acc 1.000, time 0.2 sec\n",
      "epoch 63, train loss 0.012190, train acc 1.000, valida loss 0.028094, valida acc 1.000, time 0.2 sec\n",
      "epoch 64, train loss 0.011851, train acc 1.000, valida loss 0.027094, valida acc 1.000, time 0.2 sec\n",
      "epoch 65, train loss 0.011119, train acc 1.000, valida loss 0.026099, valida acc 1.000, time 0.2 sec\n",
      "epoch 66, train loss 0.010529, train acc 1.000, valida loss 0.025270, valida acc 1.000, time 0.2 sec\n",
      "epoch 67, train loss 0.011232, train acc 1.000, valida loss 0.024491, valida acc 1.000, time 0.2 sec\n",
      "epoch 68, train loss 0.010940, train acc 1.000, valida loss 0.023924, valida acc 1.000, time 0.2 sec\n",
      "epoch 69, train loss 0.009870, train acc 1.000, valida loss 0.023356, valida acc 1.000, time 0.2 sec\n",
      "epoch 70, train loss 0.009931, train acc 1.000, valida loss 0.022872, valida acc 1.000, time 0.2 sec\n",
      "epoch 71, train loss 0.009678, train acc 1.000, valida loss 0.022559, valida acc 1.000, time 0.2 sec\n",
      "epoch 72, train loss 0.009615, train acc 1.000, valida loss 0.022140, valida acc 1.000, time 0.2 sec\n",
      "epoch 73, train loss 0.010493, train acc 1.000, valida loss 0.021750, valida acc 1.000, time 0.2 sec\n",
      "epoch 74, train loss 0.009135, train acc 1.000, valida loss 0.021205, valida acc 1.000, time 0.2 sec\n",
      "epoch 75, train loss 0.009042, train acc 1.000, valida loss 0.020571, valida acc 1.000, time 0.2 sec\n",
      "epoch 76, train loss 0.008645, train acc 1.000, valida loss 0.019847, valida acc 1.000, time 0.2 sec\n",
      "epoch 77, train loss 0.008785, train acc 1.000, valida loss 0.019384, valida acc 1.000, time 0.2 sec\n",
      "epoch 78, train loss 0.008684, train acc 1.000, valida loss 0.019074, valida acc 1.000, time 0.2 sec\n",
      "epoch 79, train loss 0.008550, train acc 1.000, valida loss 0.018950, valida acc 1.000, time 0.2 sec\n",
      "epoch 80, train loss 0.008193, train acc 1.000, valida loss 0.018934, valida acc 1.000, time 0.2 sec\n",
      "epoch 81, train loss 0.008322, train acc 1.000, valida loss 0.018853, valida acc 1.000, time 0.2 sec\n",
      "epoch 82, train loss 0.007440, train acc 1.000, valida loss 0.018537, valida acc 1.000, time 0.2 sec\n",
      "epoch 83, train loss 0.007919, train acc 1.000, valida loss 0.018188, valida acc 1.000, time 0.2 sec\n",
      "epoch 84, train loss 0.007159, train acc 1.000, valida loss 0.017618, valida acc 1.000, time 0.2 sec\n",
      "epoch 85, train loss 0.007471, train acc 1.000, valida loss 0.017181, valida acc 1.000, time 0.2 sec\n",
      "epoch 86, train loss 0.006750, train acc 1.000, valida loss 0.016803, valida acc 1.000, time 0.2 sec\n",
      "epoch 87, train loss 0.007218, train acc 1.000, valida loss 0.016442, valida acc 1.000, time 0.2 sec\n",
      "epoch 88, train loss 0.007490, train acc 1.000, valida loss 0.016046, valida acc 1.000, time 0.2 sec\n",
      "epoch 89, train loss 0.006813, train acc 1.000, valida loss 0.015523, valida acc 1.000, time 0.2 sec\n",
      "epoch 90, train loss 0.006364, train acc 1.000, valida loss 0.015034, valida acc 1.000, time 0.2 sec\n",
      "epoch 91, train loss 0.007191, train acc 1.000, valida loss 0.014708, valida acc 1.000, time 0.2 sec\n",
      "epoch 92, train loss 0.006799, train acc 1.000, valida loss 0.014473, valida acc 1.000, time 0.2 sec\n",
      "epoch 93, train loss 0.006282, train acc 1.000, valida loss 0.014325, valida acc 1.000, time 0.2 sec\n",
      "epoch 94, train loss 0.006072, train acc 1.000, valida loss 0.014196, valida acc 1.000, time 0.2 sec\n",
      "epoch 95, train loss 0.006054, train acc 1.000, valida loss 0.014017, valida acc 1.000, time 0.2 sec\n",
      "epoch 96, train loss 0.006046, train acc 1.000, valida loss 0.013729, valida acc 1.000, time 0.2 sec\n",
      "epoch 97, train loss 0.006168, train acc 1.000, valida loss 0.013378, valida acc 1.000, time 0.2 sec\n",
      "epoch 98, train loss 0.006499, train acc 1.000, valida loss 0.013021, valida acc 1.000, time 0.2 sec\n",
      "epoch 99, train loss 0.005505, train acc 1.000, valida loss 0.012718, valida acc 1.000, time 0.2 sec\n",
      "epoch 100, train loss 0.006027, train acc 1.000, valida loss 0.012473, valida acc 1.000, time 0.2 sec\n",
      "epoch 100, loss 0.0060, train acc 1.000, time 21.8 sec\n",
      "-------- Training Finished-----------\n"
     ]
    }
   ],
   "source": [
    "for index_iter in range(ITER):\n",
    "    print('iter:', index_iter)\n",
    "    \n",
    "    np.random.seed(seeds[index_iter])\n",
    "    \n",
    "    np.random.seed(seeds[index_iter])\n",
    "    # train_indices, test_indices = select(gt)\n",
    "    train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "    _, total_indices = sampling(1, gt)\n",
    "\n",
    "    TRAIN_SIZE = len(train_indices)\n",
    "    #print('Train size: ', TRAIN_SIZE)\n",
    "    TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "    #print('Test size: ', TEST_SIZE)\n",
    "    VAL_SIZE = int(TRAIN_SIZE)\n",
    "    #print('Validation size: ', VAL_SIZE)\n",
    "\n",
    "    print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
    "    x_train,y_train, x_val,y_val, x_test,y_test, all_data, gt_all = geniter.generate_iter(\n",
    "            TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
    "            total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
    "            INPUT_DIMENSION, 16, gt)  #batchsize in 1\n",
    "\n",
    "    del all_data,gt_all\n",
    "    gc.collect()\n",
    "    \n",
    "    band=x_train.shape[-1]\n",
    "    patch=x_train.shape[-2]\n",
    "    \n",
    "    \n",
    "    x_train=torch.from_numpy(x_train).type(torch.FloatTensor) \n",
    "    y_train=torch.from_numpy(y_train).type(torch.FloatTensor) \n",
    "    Label_train=Data.TensorDataset(x_train,y_train)\n",
    "    \n",
    "    del x_train,y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    x_test=torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "    y_test=torch.from_numpy(y_test).type(torch.FloatTensor) \n",
    "    Label_test=Data.TensorDataset(x_test,y_test)\n",
    "    \n",
    "    del x_test,y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    x_val=torch.from_numpy(x_val).type(torch.FloatTensor)\n",
    "    y_val=torch.from_numpy(y_val).type(torch.FloatTensor)\n",
    "    Label_val=Data.TensorDataset(x_val,y_val)\n",
    "    \n",
    "    del x_val,y_val\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    label_train_loader=Data.DataLoader(Label_train,batch_size=64,shuffle=True)\n",
    "    label_test_loader=Data.DataLoader(Label_test,batch_size=64,shuffle=False)\n",
    "    label_val_loader=Data.DataLoader(Label_val,batch_size=64,shuffle=False)\n",
    "    \n",
    "    del Label_train,Label_test,Label_val\n",
    "    gc.collect()\n",
    "    \n",
    "    model = ViT(\n",
    "    image_size = patch,\n",
    "    image_band = band,\n",
    "    num_classes = CLASSES_NUM,\n",
    "    dim = 64,\n",
    "    depth = 5,\n",
    "    heads = 4,\n",
    "    mlp_dim = 8,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    mode = 'ADCF'\n",
    "    )\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=5e-4,\n",
    "        weight_decay=0)\n",
    "\n",
    "    \n",
    "    tic1 = time.time()\n",
    "    train(\n",
    "            model,\n",
    "            label_train_loader,\n",
    "            label_val_loader,\n",
    "            loss,\n",
    "            optimizer,\n",
    "            device,\n",
    "            epochs=PARAM_EPOCH)\n",
    "    toc1 = time.time()\n",
    "    \n",
    "    \n",
    "    pred_test = []\n",
    "    tic2 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for X, y in label_test_loader:\n",
    "            X = X.to(device)\n",
    "            model.eval()\n",
    "            y_hat = model(X)\n",
    "            pred_test.extend(np.array(model(X).cpu().argmax(axis=1)))\n",
    "    toc2 = time.time()\n",
    "    collections.Counter(pred_test)\n",
    "    gt_test = gt[test_indices] - 1\n",
    "\n",
    "    overall_acc = metrics.accuracy_score(pred_test, gt_test[:-VAL_SIZE])\n",
    "    confusion_matrix = metrics.confusion_matrix(pred_test, gt_test[:-VAL_SIZE])\n",
    "    each_acc, average_acc = record.aa_and_each_accuracy(confusion_matrix)\n",
    "    kappa = metrics.cohen_kappa_score(pred_test, gt_test[:-VAL_SIZE])\n",
    "    \n",
    "    torch.save(\n",
    "        model.state_dict(), \"./models/MyTransformerPatch_\" + str(img_rows) + '_' +\n",
    "        Dataset + '_split_' + str(VALIDATION_SPLIT) + str(round(overall_acc, 3)) + '.pt')\n",
    "    KAPPA.append(kappa)\n",
    "    OA.append(overall_acc)\n",
    "    AA.append(average_acc)\n",
    "    TRAINING_TIME.append(toc1 - tic1)\n",
    "    TESTING_TIME.append(toc2 - tic2)\n",
    "    ELEMENT_ACC[index_iter, :] = each_acc\n",
    "    \n",
    "    del label_train_loader,label_test_loader,label_val_loader\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"--------\" + \" Training Finished-----------\")\n",
    "record.record_output(\n",
    "    OA, AA, KAPPA, ELEMENT_ACC, TRAINING_TIME, TESTING_TIME,\n",
    "    './report/' + 'MyTransformerPatch_' + str(img_rows) + '_' + Dataset + 'split'\n",
    "    + str(VALIDATION_SPLIT) +'.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  (156, 13, 13, 145)\n",
      "Test size:  (2936, 13, 13, 145)\n",
      "Validation size:  (156, 13, 13, 145)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train, x_val,y_val, x_test,y_test, all_data, gt_all = geniter.generate_iter(\n",
    "            TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
    "            total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
    "            INPUT_DIMENSION, 16, gt)  #batchsize in 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del whole_data,padded_data,x_train,y_train, x_val,y_val, x_test,y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all=torch.from_numpy(all_data).type(torch.FloatTensor)\n",
    "y_all=torch.from_numpy(gt_all).type(torch.FloatTensor)\n",
    "Label_all=Data.TensorDataset(x_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_all,y_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_loader=Data.DataLoader(Label_all,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Get classification maps successful-------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 122.88x708.48 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIkAAALSCAYAAADp8vLUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfZklEQVR4nO3de5CU9Z3v8Xdf5sowXAYYZpQZ5BLlIqAoiiIY78bFqMRkQ9RoTGorpyqb7GapmFR23WRPZcsym2xSG7XWRBNNGY6XsKSOrmGVDRswSIxGlBjxKCIXQRguwtxn+nf++M0ICPS3Z6a7n0t/XqkumaGn+we88/Rz+T3Pk3DOIZJNMugBSPgpEjEpEjEpEjEpEjGls/1mIpHQpk+JcM4lTvZ7WpKISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGISZGIKeSRDAPGAh8BygIeS+nKevPo4PUA5wFzgI3A74A9QQ6oJCWcO/lNxMNxh/FyYHjfr9uA9gDHEl/Z7jAegUikGHQbehkSRSImRSImRSImRSImRSKmPERSO/SXkFAbYiT9u8wlzrQzTQDtTJMhUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiCnEkC4HKoAchhDqSMdiRVOKnKqQKP5wSFuJIngIOGM8ZASwB/qLgoyllEZ9PUgPMxJ8C2pbD86cCLcC+Qg4qknQG3wcW488vXgX0BjyWcCmBSUfDgbM5ft0k0fd75X1fb8AvTRYD807wfDmRmERyGGjlSAz9ksBkYErf1+8B9wLbgdlABf6SFjH5ayiQEvm4SQAf/qPUA/vxl7V4F9hW5DGFSwl83FhO1Ppu/JLnMHAm2idzcjFbkgwHDgU9iEgqoSVJzP44IRGzJYkMVgktSQbqo/iL90k2Jb4kqSa3PbXxpyXJSSmQXJR4JJILRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRSImRRJRp3AKwxlOmnTB30uRRNROdjKJSVzP9TTSWND3KvFry0ffXOYyn/lsZCPrWEfvIG9AqWvLR1iCBPXUk+DE/4Z/4A+00MJCFlJLbUHGoEhC7kzO5HN8jr/kL2mi6YSxvMzLpEgxgxkFGYMiCbktbOG3/JaxjOVmbuZczj3uOVvZygEOMIIRBRmDIgm5QxxiLWt5hEc4yEEu53ImM/mY57TTTjfdBRuDIomIvezlLd4iTZr5zC/qeyuSCHmJl+iggwMcOOb7DkcvvWTIFOR9tQkcIUmS1FBDG2300HPM7zXTzEEOHhdQrrJtAisSAbSfRIao8Dv+JVTSpBnOcMCvy7zP++a6jCIpMQ00sJSllFFGhgxP8zQv8mLWn9HHTYnZxjZWs5o9ff9rocX8Ga24lqj+3fsO/0+sFdcSNoc51FBz3Pdd3/9yoUhiLE2amcxkMYuH9DqKJMZqqKGZZmqpPelUg1xo6ybGuujiVV7lNV7L+aPlRLTiKoBWXGWIFImYFEmJSZEiOcB/dkVSYhpp5EIuZCQjqaIqp5/RimuJSZBgKUuZyET2sIeHeZh22rXiKkc4HE/yJHvYQw01pEiZP6MlSYmqpZZqqtnFLkAz0yQH+riRIVEkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYtIpFXExYgTMmQNTp0J3N/z+9/Duu7B//7HPa2ryz1m7Fjo7c3ppRRIHY8bAjTdCff2R7512Grz/Pjz/PKxb57/X2Ag33QRlZZBOw69/ndPL6+MmDvbtg0TfdJCj5wfV1vqlS7Lvn7m83AcCcMopUFGR08srkjhx7sSx9Hv/fTh82D9n/Hiors7pZfVxEwfOwfbtMG7cka8TieND2bcP1qyBkSNh7144cCCnl1ckceAcbNgAzc0watSRj5euLnjrrWNjeeGFAb+85rjGSWUlzJ4N554LO3f6cHbuhIx9fVdNhC416bQP4+g4Ro6Emhr/sXQC2SLRx00c9Rx7IWBGjICPfcyvrK5fD889N6CX09ZNKTjrLL8DrbYW5s/36y0DoCVJ3J16Kpx9NrS0+J1nzkFb24BeQpHEUTIJCxf6rZ0xY/xOtD//Gd54Y1Avp0jiautWv5v+N7/xK6u7dw/6pbR1I4BO85QhUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiSloAwYP/gfVySlYAqwBHK8eedxgokkcdRDCmsEcA0wFjhzcC9R3DP4UsBUYD5QCbQBvwC6ijqK0nIYeBe/NLkM2A7sHNhLFG9JMgL4LPAJ4FSgF3gJ6C7aCEpTL7C179dlwBVAbpdK+0DxIikHhuOjWAU8BGwEdCJp4W3Gx5IAmoEZA/vx4p4LXI3PshXFUUxJYCkwGR/Ke8BP8R/3fcJzLnAb/jNSgRRXBtjW92uHX4kdwNIk+E3gBDAHWIT/SJLC6Lt65wdblRNy/9Hgr09SA1wFVOAXg68FO5zYSjPoXQ7BR9KBXxROJbDlWl0dpFL+ylETJ/rvtbbCf/1XTle3DL8RHLvHNQO8nvuPBx9JN7Aev/hrL/7bz5gBixf7K0YlEkeuur1vHzzzTPHHUxBjgdq+Xzvgz8D/y/3Hg48E/Hb8z/E7eoookYAFC/w1co+WZYMvmuZ86Ou1QG53MQHCsOIK0EPRAwEfw/r1/rYvzvlH/xLkiSegt7f4YyqYBH4psgPYO7AfDceSJEAvvwy7dvl7BTU3w+rVPpTY6V86bmfAh0FKPhLwFybcvdvfbCq2+pckmwb+o+H4uJHC2oXfQHgBf7BvgIq2W3468DbH7AmWYknhj5sdxq//nUDgu+VHAJcDo4vxZnK8XuAAJw3EUpRIxgMjgXOK8WYlKJXK+XZ6g1KUSPrj2Jb1WTJYU6b4O5UkC/SvWZRI0vj1kT8V481K0Omnw7Rp/rY2hVDwSEbjt7weJ5C97rE3fjyccYa/WdbUqYV5j4JGMgpY0Pfr1kK+UQk77zyoqvIfNc3NhfnIKWgklwFnAa8W8k1KWEWFv2l4/0HJqVP9bfbyraCR9G94x3Evdxg0Nh57K+B0GqZPz//7FHS3fAt+DtEgdvLJAPVPc6ittZ87UAWNZA1+UaXTagrv6DvQ51tBP256UCCFdOiQf0DhAgEd4Iu0vXvh8cdh82Z/x/lMpjBzYHQPvhhIp2HYMJg5EzZuPLJ0GQjdhl5MgR8FjoNqoAmYHfRAAqCZaTm6DpgEHATexE/NKBVakuQo3fcYxaCvBRNZiiSPzgGmBT2IAlAkA+Dw0x0OnuT3G4GFxO8CTlonydEq/MWZdnLyHYQH8Sfrn0q8JlhFahP4lFNOYcKECZxzzjk0Njby6KOP8sc//jHoYX2gFvgc/oDm40Rr0ncs9pOkUimWLVvGVVddBYBzjq6uLu644w5eeumlQMdWhr8+zC5gGPApYCV+KygqYrGfZPLkySxYsOCDrxOJBOXl5Xzyk58McFTeucAngdPwZ1HuBOYGOqL8ikwkM2bMIJVK0d7eTkdHB845EokEw4YNC3poVODXU1qO+t5U4hNKZFZcn376adauXQvAuHHj+N73vkc6nWbFihUBj8yfGNeJP802gT8Xqgx/JdLXif6Ot8hE0t7eTnu7n0qdSCTo6uri3Xff5bnnngt4ZHAI6B9FFf6qXr34c41qUCSB2LNnD6tWraKjo4POzgFcaKMI2vGXWjkDv19ld7DDyYvIbN18WFlZGZlMht5YXUQkOLHYBJbCisUmsARHkYhJkYhJkYhJkYgpsEgShTxRJCb6996mCHaOSmA706ZNm8aUKVPIZDK0t7fz3HPPkclk6O7uJhOLa3UPXgJ/Eedz8fNqwV+fN6hj3YFFMmXKFL7yla+QSCTo7e2lpcUfHlu3bh3Lly/nvffeC2pogTk6jhn4XfwJ/MHD9wMcV2AfN1VVVR985KTTaerr66mvr+faa69l/Pgh3J80oirwUx9vw0dShb83QyvwBkfukBaEQI/dJBIJ+vf49vT0sGrVKl555RU2bRrEFWkjKom/Fc11+KWIw186bDP+urxd+EuwDvLCiXkRWCSbNm2ira2N6upqenp6eOyxx/jJT35CT0+Qfx3FlQQuxE8p6MTPi12Pv4FEmP4WAjt2k06nufLKK7n++uvZsGEDDzzwQEkFAv56cjcA6/AfKQ4/xSAIoT7Al0qlyGQyZBtHXI0GGhjU5d7zLlskgc8nKeVD/T1EY0Z94EsSCQdNFZAhUSRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiUiRiCjySVNADEFOgkSSA2/GzxCW8Ap2ZVge8iJ/Cdxt+prgEI7Qz05LAcPz1xf4PcGaQg5GTCnydpF8TPpbQSAP1QQ8iHAKN5BD+nJNQasJfd/PbwISAxxKwQCPpAO4EvgT8kWBPij7Oe8Dv8Odf/hhoDnQ0gQrNKRUpIIM/iy00khy5MEjoBpdfoT45q18oT9E67jIplfgN9iT+yvHdwAH8ad0vENI/xZCFZkkSDVUcuzb7Pv7uNtEX6nOBJRxCu59EokGRiEmRiEmRiEmR1OH35l0d9EDCq7QjmQ48D/xv/MXLoiCN311T5LcsTVXA3wBjgL8CfhnscHL2v/CXZ/xV8d6ydJck5+MnsTyCn6dwstuGh81c4HqKeh3x0o0kid9h+kOKdkxm2rRpXHvttSSTQ/xrPw9/O/MiKd1IDgI/Af5cvLccN24cN998MzU1NYN/kYfxK9s/yNeobKUbyQvAsuK9XSKRYPr06ZSXlw/thZ7Bh/LxvkcRlG4kRVZeXs6iRYtYt24dra2tQ3uxO/Errg9SlDmfpbt1U0SJRIIlS5ZQV1dHS0vL0K9d24rfytkDnAO8MvQxZqNIiiCVSnH11VfT1dXFs88+m58XbQX+Lj8vZVEkRZDJZFi9ejW7d+/mnXfeCXo4A6b5JAJoPokMkSIRU0lGUgFM7Puv2EoykmnA74HvoFByUZKRvAysBv4af3NEya5kt27m4fdwtwAXAduDHU7gtHVzAhuAn+MPpg4PeCxhV7JLkn6z8bdXbQ96IAHTyVli0seNDIkiEZMiEZMiEVO0pwqk07B0KZSXw0MPQVdUprxHSzS3btJpmDEDPvMZaGmB1lYYMwa+8x2FMkjx2wS+5RaYNg1+9jPYvNl/74tfhOpquPvuYMcWUdkiwTl30gf+jJTwPc44w1FWduz3EgnHzJnBjy2ij2wdRHNJInmnnWkyJIpETIpETIpETIpETIpETIpETIpETIpETNE+ChwC1wCL8bPu/xF/34q40W75IWgEHgMuALbgr/jZEeiIBk+75QugAngImN/39W5OcHucmFAkg5QCpuKvlPkKcAvRucrnQCmSXNTWwtlnH/OtDuB7+EuXfQJ4I4BhFYvWSXJRUwMrVsDGjfDVrwY9moKI36SjIB6XXOLYvNlx/fXBj6XIk44USbbHggWOyZOPfH3TTY7nn3fMn3/i5ycSjvLy4Med50i0nySbW26BqVPhiiuguxt++UuYOBEOHPATsRct8s9zDv77v2H3bvjnf/YTsiN4Ab2T0pIky+OccxwtLY6FC0+8lHnyScdddzn+4R8co0b5748e7aisDH7seVySaMU1m1QKLrsMfvMb6Ow8/verqqA9HtcjiN8pFZJ32uMqQ6JIxKRIxKRIxKRIxBSbSMbi53b8Y8DjiKPY7HE9C380No4zw4aq/8aQmczgZrzEJpIMfj5HT9ADCZGKigrmzJnD2WefTWtrKytXruTgwYMDfp3YRLIOP9d0Z9ADGazLL/fXWtm6NS8v19TUxAUXXMDkyZN5++23Wbt27aACAe1xDY/Zs/1clRUrYOVKGMRHQyqVorKyklmzZrFgwQKcc6xcuZItW7bQ05N9Gavd8lExahTccAMMGwZPPQVvvZVzLHV1dVx44YWcfvrpVFVV0dbWxooVK3jzzTdz+nlFEibV1dDRkf0ff84c+Oxn4ZVX4Kc/zSmUVCpFY2MjTU1NAGzfvp2tA/jo0sy0MD1mzXJ89av2dILKSsd3v+toaCjKuLJ1EJv9JJGxcaO/QuTFF2d/Xmcn7NgByeD/ifI2gibgqny9WNw98QQ0NGR/TkUFNDfDvn3FGVMWeYlkAvAo/mbXkoOWFnjmGft5W7eGYlLTkCOZgN8dPm/oYykdnZ2wbVv252QysD0k9/Ma6orr7eAy4B4GNyHolUI9wrni2r9xth0w/r8hETXk3fL/F/gR8NLQxyIFkCZNkiQLWECSJM+Qw7rQh2hnWoxMYxoXcdEx37uSK5nJTBppZAc7uJiL2cWu434228602BzgE1jEIu7lXgDaaWcNa3iN15jCFIYxjElMYiITTxhJVtrjGp/HhVzoDnDA9dLrvsE3XDXVDnDncZ5ro83dxV2ujLIT/qzOBS6RRxll7n7ud4/wiBvBiA++nyLlPs2nXTPNJ/1ZncFXQsopB6BrgJfU0VFgMekMPhkSRSImRSImRSImRSImRSImRSImRSImRVJMiYQ/t6aqKuiRDIiOAhdLVRVcdBFMmQKvvgqHD8OLLwY9qpxoSVIst9wCM2fCz37mz/mdPTvoEeVMkRRDWZkP5N//HVpbgx7NgCmSYjl48Mi1YHt64LzzoLw82DHlSJEUQ0+Pv6T49On+67174fXX/cWEm5vh3HODHZ9BkRSDc/DjH8OnPuU/evq/B/5eOuPHBze2HCiSYunshLY2GDPGf/3YY0c+fr79bfjBD4Ibm6Vkpi9OnuxvNRLkGCZOdNxxx7G3O6mqcsyb55g+PdCx6aoCANddd2RRH5R33oFdu/xKa7/2dtiwAf70p+DGZdDOtGLKZPxFaSKmdJYk1dUweXLQo4ik0olk5Uo466zgP3IiqHQi2bTJ75e4804YPTro0URLyWzdgCOZdHz8446HHjr2Box66OSs41x8MTgHa9YEPZLQ0Hk3H9atK9APROlsAo8aBV/4AgwfDr/7XSSPxgaldJYk+/fD3Xf7C+ief37Qo4mU0lsnGT7cHz/Zv9/fBFqAElkn+QywFrgVGJbtiYcOwZYtCmQAYrEkOQtYAwzH3/NmFvB6oCOKntgvSVr7HlIYsYhkM/Bz4DDwJ2B/sMOJnVh83ABU4G/W2AYEfzX26NGVjsQU+3WS0pIGvgDcBRRn6kPp7HGNjSnAD4FK4Hkgt9unDYWWJJFSBiwF+s/X+auivKsiiZQm4O848s/2q6K8qyKJlETfA+AZYHlR3lXrJJGyA7gPqAf+GmgpyruGchN43jyoqzvydW8v/Pa3RbrT2KWX+rmw3/++f+OCSuAnhgUvcrd8/eEPcT09R4bS04P72tcK/L6LFzvGjXOk045lyxw33FDA90s4mOPgfgdXBDZl8ehH5E7OuuMOf6Pt3bv916kU1NcX8A2nTYOlS/2MtZ4eeOQRmDv3xDPrZ87Mwxt+EX9ryw7ghTy8XmGFMpK2NvjVr3wsOd6FffBOO83PoF+50s8xAV9nQwP8/d/7S1j1a26Gm27Kw5suBy7Hr1dE4CBCGD9u+h/DhuG++U3cm2/ili4t0PtUVztGjz7++7fd5tiyxXHGGY5Uyv/3Rz9yXHRR4B8NhXhE/n43dXW4ZLLI75tKOR54wPGJTzi+/GXHu+86vvENv84Sgr+TYkYSyq2b0Lj1VnjvPZg0yU97vPtuv84SQzoKPFiVlT6KTMavmxR8kzg4Ogo8WB0dRyLpD6Smxq/UlhBFMlAjR/qtoSuvzPKUkVx66aU0NzcXb1wFpI+bwUgmwa/1H/dbF1xwAfPnz6empobDhw/z4IMPsm9f+Ddzs33c6NjNYGTZedPQ0EBNTQ2JRIKamhoqKyuLOLDC0MdNnq1Zs4Z33nmnfxcCs2bNCnhEQ6ePmwJobm7m/L5TSdevX8/WrVuzPr+uro6WluIc0T2ZyB3gK7XHkiVL3IQJEwIdQ+QO8JWSuro6pk6dSkVFRdBDOSlFErDOzk5aQ34ZjFBG0kQTN3ETKVJBD6XgDh8+zLZt2+gN897cMK6TNNHkNrPZ3cd9bgYzXIpU4OsNhXyUl5e7ZDIZ2nWSUEYCuDu502XIuIMcdF/my7EPJehHJCO5mqs/GEk77e42bgv8LzLOj8hv3ZRTTgOldVAtTEK7M62eeuYzH4BeenmWZ2mjLajhxJ7mkxzlkkv8nc7+8IegRxIukZpPMnbsWG699VbmzJnDpEmTmDRpEqeddhrp9NCPRV5yCfziFzB2bB4GWkAJEpzP+VQRjvsHh+4ocCaTYcyYMVx33XXHfG/btm0sX76c9kGeoVVTA9/8JqTTfkZimKVJcw/3sIQlbGFL0MMJ35KkpaWFxx9/nEOHDh1Zu04mqa+vJ5kc3HBrauDee2HhQvj618N3z+aRjORGbqSBBsoo4wquoJlmHOH4tA/tOsmIESOYO3cuNTU1AHR3d7N69Wo6++5bV0YZSZJ00pn1dYYPh3vu8edevfGGv2nVwYMFH/6AVFLJ3/K3nMqpnM/57Gc//8F/cB/30U1xLnEey6PA3+Jb7kmedEmy76lctgzX2+v/SBs34lKp4Md+skcllW4iE91YxoZqP0no1kly9SAPsoc95iK5rAy6uvzE97DroIO3eTvoYRwntB83+VJdDZddBrNm+RXW++/nRFNTS572k4gpUvtJJHwUiZgUiZgUiZgUSQDKy2HRImhsDHokuVEkAXAOPv95+NKX/BmjYReBIcZPd7e/1Nepp2Y97zw0FElAduyAf/s3qArHbICstDMtQGVlfv0kDKfdaI+rmLTHVYZEkYhJkYhJkYhJkYhJkYhJkYhJkQTs8sv9DZ+WL4ePfQxGjQp6RCcQ1dnycXnMm4d7+21cJuMfmzbhPvKR4o8j8lcViLMNG+DGG+Gpp/zR4dNP9+cKhYqWJOF4VFbi1q/3S5N779WSRE6gowMOHPA3wwjbeklkT86Ko2XL4Pnn4dVXgx7JsXQUWAB0FFiGRpGISZGISZGISZGIKQKbwGXA3wAbgW7gf/r+K8USgU3gcmAl0AZMAv4TuBOFkl8Rny2fAOqAvUAz8C1gFfBIkIOKnYhH8mFNwMPAFWBcVE9yF7OdaZ8BpgPTgh5IyYhYJKOBi4ExwNxgh1JCIhbJ7cBlwDbgpYDHUjoitk5yLnA+sB74fcBjiZeYrbhKIcRsxVWKTZGISZGISZGIKRKRJPE754MfRWkK/Z98KvAM8CPgL/CH+4orAZwNrMDvxCs9oY4kAVwNvA48CLwG9BR1BLXAl4EngMXAjUV997DQfpKsyoDZ+ONFV+EPKF4BhPwmfoOgnWl5MRr4Gn6KwssBjyX/FEnelAEZoDfogeRdtkgiMH0xTEpzNlyoV1wlHBSJmBSJmBSJmBSJmBSJmBSJmBRJ7FSR79NNFEnkTcUfH6/AB/Iv+LMcK/L2DtrjGnk3AncAzwO7gU8B+/DB5OcMR0USed/FH3z8PDCi73vD8CevPZuXd9DHTeR1AV8HrgS29H1vGPA5/AHJoVMkeXY7t7OIRUV+1278x833+77eDtxDvg5IKpI8u4zLWM5yLubiAN69G9gBfBpYl7dXVSR5totd1FPPcpYzrehXPvg1cA2wNr8vq2vL5/dRS617lEddhoxbwILAx5PrI2sHiqQwoXyUj7oRjPjge0mSbh7zXDnlgY9PkYT0UUWVe4M33L/yr24KUwIfjyIJ4SNBwv0T/+QyZNwOdrg7udOlSAU+LkUSskcDDW41q12GjNvDHjeDGYGPSZGE8HF0KPdwT2jWURRJyB79obTT7q7hmsDHY0Wi824C0kADS1jCAzxAG21BD0cnZ4lNl8OSIVEkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYlIkYko454Ieg4ScliRiUiRiUiRiUiRiUiRiUiRi+v8cMEc/A+ExOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 122.88x708.48 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Utils.generate_png(\n",
    "    label_all_loader, model, gt_hsi, Dataset, device, total_indices,\n",
    "    './classification_maps/' + 'MyTransformerPatch_' + str(img_rows) + '_' +\n",
    "    Dataset + 'split' + str(VALIDATION_SPLIT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
